% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/abcnn.R
\name{abcnn}
\alias{abcnn}
\title{Create an \code{abcnn} R6 class object}
\value{
A \code{R6::abcnn} object

an \code{abcnn} object that can be used to fit(), predict() and plot predictions
}
\description{
\code{abcnn} constructs a \code{R6} class object for parameter inference with ABC and neural networks.
It implements four different method mixing ABC and neural networks implemented in R \code{torch}.

The \code{initialize} function (\code{abcnn$new()}) takes as arguments three data frames of training summary statistics,
training theta values and observed summary statistics. Public slots can be accessed and modified.
A new \code{abcnn} object is created with \code{abcnn$new()}.
}
\details{
Four methods are available for parameter inference. The two core methods are \verb{concrete dropout},
an implementation of Gal et al. (2017), and \verb{deep ensemble}, an implementation
of Lakshminarayanan et al. (2017), that allow to estimate both the aleatoric and epistemic uncertainty
for each sample. \verb{monte carlo dropout} is an implementation of Gal and Ghahramani (2016),
that provides a simpler model that is easier to train, despite its limitations (the dropout rate must be arbitrary chosen).

A fourth method is \code{tabnet-abc}. This is a new method, combining regular ABC inference with the \code{abc} R package,
and a Tabnet neural network, as in Arik et al. (2021) and implemented in the \code{tabnet} R package.
This is the same idea than in Ã…kesson et al. (2021) or Jiang et al. (2017), except than te MLP/CNN used to estimate summary statistics
is replaced by a \code{tabnet} model specifically designed to handle tabular data and feature selection through
an attention map on features. The \code{tabnet} neural network is trained to predict summary statistics from the observed summary statistics.
Then these predictions are used as a supplementary set of summary statistics and regular ABC inference is performed on it.
Explain methods are specific the \code{tabnet-abc} model.

In addition, the credible interval is calibrated with conformal prediction, as in Baragatti et al. (2024).
As it requires a proxy of uncertainty, conformal prediction is only available for \verb{concrete dropout},
\verb{deep ensemble} and \verb{monte carlo dropout} (only for the epistemic uncertainty for this last method).

The neural networks are implemented with the \code{torch} R package and support CUDA devices for training.
The \code{luz} package is used as a higher level API for training and predictions with \code{torch}.
The device (\code{CUDA} or \code{cpu}) is automatically detected by \code{luz}.

The \code{abcnn} object has public methods to perform each inference step and visualizations.
\itemize{
\item \code{new()} to create a new \code{abcnn} object
\item \code{fit()} to fit a neural network
\item \code{predict()} to compute conformal predictions from the fitted model
\item \code{summary()} to print a summary of the \code{abcnn} object
\item \code{predictions()} to print predictions
\item \code{plot_training()} to plot the training curves
\item \code{plot_prediction()} to plot all predictions with their credible intervals
\item \code{plot_posterior()} to plot the prior and posterior distributions, with the mean and credible intervals, of a single sample
}

The hyperparameters of the neural network can be configured in the \code{new()} method
or modified directly in the corresponding public \code{slot}.
}
\section{Slots}{

\describe{
\item{\code{theta}}{parameters of the pseudo-observed samples (i.e. simulations)}

\item{\code{sumstat}}{summary statistics of the pseudo-observed samples (i.e. simulations)}

\item{\code{observed}}{summary statistics of the observed samples}

\item{\code{model}}{the \code{luz} model}

\item{\code{method}}{the ABC-NN method used}

\item{\code{scale_input}}{the scaling method for summary statistics}

\item{\code{scale_target}}{the scaling method for targets (i.e. theta)}

\item{\code{num_hidden_layers}}{number of hidden layers in the neural network}

\item{\code{num_hidden_dim}}{number of hidden dimensions (neurons) in each hidden layer}

\item{\code{validation_split}}{proportion of samples retained for validation at the end of training}

\item{\code{num_conformal}}{number of samples retained for conformal prediction}

\item{\code{credible_interval_p}}{significance level for the credible interval, between 0 and 1}

\item{\code{test_split}}{proportion of samples retained for test at each training iteration}

\item{\code{dropout}}{dropout rate}

\item{\code{batch_size}}{batch size}

\item{\code{epochs}}{number of epochs for training}

\item{\code{early_stopping}}{whether to do early stopping}

\item{\code{patience}}{patience hyperparameter for early stopping. See \code{luz::luz_callback_early_stopping()}}

\item{\code{callbacks}}{custom callbacks}

\item{\code{verbose}}{whether to print messages}

\item{\code{optimizer}}{\code{torch} optimizer nn module}

\item{\code{learning_rate}}{learning rate}

\item{\code{l2_weight_decay}}{L2 weigth decay (regularization)}

\item{\code{variance_clamping}}{(min, max) values for variance clamping during training}

\item{\code{loss}}{\code{torch} nn loss function}

\item{\code{tol}}{tolerance rate for \code{abc} functions (only for \code{tabnet-abc})}

\item{\code{abc_method}}{ABC sampling method in \code{abc} function (only for \code{tabnet-abc})}

\item{\code{num_posterior_samples}}{number of samples to generate for the posterior distribution}

\item{\code{prior_length_scale}}{prior length scale hyperparameter value}

\item{\code{wr}}{\verb{concrete dropout} regularization term for weights}

\item{\code{dr}}{\verb{concrete dropout} regularization term for dropout}

\item{\code{num_networks}}{number of networks in \verb{deep ensemble}}

\item{\code{epsilon_adversarial}}{the amount of perturbation for adversarial training in \verb{deep ensemble}}

\item{\code{device}}{\code{luz}/\code{torch} device for tensors}

\item{\code{input_dim}}{number of input dimensions of the neural network}

\item{\code{output_dim}}{number of output dimensions of the neural network}

\item{\code{n_train}}{number of training samples}

\item{\code{sumstat_names}}{names of summary statistics}

\item{\code{output_names}}{output names}

\item{\code{theta_names}}{names of theta to estimate}

\item{\code{n_obs}}{number of observed samples}

\item{\code{prior_lower}}{lower boundary of priors (for figures)}

\item{\code{prior_upper}}{upper boundary of priors (for figures)}

\item{\code{fitted}}{the fitted \code{luz} model}

\item{\code{evaluation}}{the evaluation metric}

\item{\code{eval_metrics}}{\code{torch} nn metrics for evaluation}

\item{\code{posterior_samples}}{array of posterior samples}

\item{\code{quantile_posterior}}{quantiles computed on the posterior samples, given the \code{credible_interval_p}}

\item{\code{predictive_mean}}{values predicted by the model for each observed sample}

\item{\code{aleatoric_uncertainty}}{aleatoric uncertainty for each observed sample}

\item{\code{epistemic_uncertainty}}{epistemic uncertainty for each observed sample}

\item{\code{overall_uncertainty}}{overall uncertainty for each observed sample (epistemic + aleatoric)}

\item{\code{epistemic_conformal_quantile}}{the quantile factor to get the conformalized credible interval for epistemic uncertainty}

\item{\code{overall_conformal_quantile}}{the quantile factor to get the conformalized credible interval for overall uncertainty}

\item{\code{dropout_rates}}{the dropout rate hyperparameter estimated by concrete dropout}

\item{\code{input_summary}}{statistics computed on input data (for scaling)}

\item{\code{target_summary}}{statistics computed on target data (for scaling)}

\item{\code{sumstat_adj}}{adjusted training summary statistics after scaling}

\item{\code{observed_adj}}{adjusted observed summary statistics after scaling}

\item{\code{theta_adj}}{adjusted training theta after scaling}

\item{\code{calibration_theta}}{adjusted theta for conformal prediction after scaling (calibration set)}

\item{\code{calibration_sumstat}}{adjusted summary statistics for conformal prediction after scaling (calibration set)}

\item{\code{ncores}}{number of cores for parallelized steps}
}}

\examples{
\dontrun{
# Load test data
df = readRDS("inst/extdata/test_data.Rds")

theta = df$train_y
sumstats = df$train_x
observed = df$observed_y

# Create an `abcnn` object
abc = abcnn$new(theta,
                sumstats,
                observed,
                method = "concrete dropout",
                scale_input = "none",
                scale_target = "none",
                num_hidden_layers = 3,
                num_hidden_dim = 128,
                epochs = 30,
                batch_size = 32)
 abc$fit()

 abc$predict()
 }



}
\references{
\insertRef{baragatti2024approximate}
\insertRef{gal2016}
\insertRef{gal2017concrete}
\insertRef{lakshminarayanan2017simple}
\insertRef{arik2021tabnet}
\insertRef{tabnet}
\insertRef{aakesson2021convolutional}
\insertRef{jiang2017learning}
}
\seealso{
\code{\link[R6:R6Class]{R6::R6()}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-abcnn-new}{\code{abcnn$new()}}
\item \href{#method-abcnn-fit}{\code{abcnn$fit()}}
\item \href{#method-abcnn-predict}{\code{abcnn$predict()}}
\item \href{#method-abcnn-dataloader}{\code{abcnn$dataloader()}}
\item \href{#method-abcnn-conformal_prediction}{\code{abcnn$conformal_prediction()}}
\item \href{#method-abcnn-predictions}{\code{abcnn$predictions()}}
\item \href{#method-abcnn-summary}{\code{abcnn$summary()}}
\item \href{#method-abcnn-plot_training}{\code{abcnn$plot_training()}}
\item \href{#method-abcnn-plot_prediction}{\code{abcnn$plot_prediction()}}
\item \href{#method-abcnn-plot_posterior}{\code{abcnn$plot_posterior()}}
\item \href{#method-abcnn-clone}{\code{abcnn$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-new"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-new}{}}}
\subsection{Method \code{new()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$new(
  theta,
  sumstat,
  observed,
  model = NULL,
  method = "concrete dropout",
  scale_input = "none",
  scale_target = "none",
  num_hidden_layers = 3,
  num_hidden_dim = 128,
  validation_split = 0.1,
  num_conformal = 1000,
  credible_interval_p = 0.95,
  test_split = 0.1,
  dropout = 0.5,
  batch_size = 32,
  epochs = 20,
  early_stopping = FALSE,
  verbose = TRUE,
  patience = 4,
  optimizer = torch::optim_adam,
  learning_rate = 0.001,
  l2_weight_decay = 1e-05,
  variance_clamping = c(-1e+15, 1e+15),
  loss = torch::nn_mse_loss(),
  tol = NULL,
  abc_method = "loclinear",
  num_posterior_samples = 1000,
  prior_length_scale = 1e-04,
  num_networks = 5,
  epsilon_adversarial = 0,
  ncores = 1
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{theta}}{a vector, matrix or data frame of the simulated theta_parameter values}

\item{\code{sumstat}}{a vector, matrix or data frame of the simulated summary statistics}

\item{\code{observed}}{a vector of summary statistics computed on the data}

\item{\code{method}}{either \verb{monte carlo dropout}, \verb{concrete dropout}, \code{tabnet-abc} or \verb{deep ensemble}, See \code{details}}

\item{\code{scale_input}}{the method to scale summary statistics before training (\code{none} (default), \code{minmax} or \code{robustscaler})}

\item{\code{scale_target}}{the method to scale the parameter to estimate before training (\code{none} (default), \code{minmax} or \code{robustscaler})}

\item{\code{num_hidden_layers}}{the number of hidden layers in the Neural Network (default = 3)}

\item{\code{num_hidden_dim}}{the dimension of hidden layers (i.e. number of neurons) in each layer of the Neural Network (default=128)}

\item{\code{validation_split}}{the proportion of samples retained for validation in `luz}

\item{\code{num_conformal}}{the number of training samples retained for Conformal Prediction (default=1,000)}

\item{\code{credible_interval_p}}{the alpha value for the quantile credible interval (default=0.95, with alpha/2 and 1 - alpha/2 quantiles)}

\item{\code{test_split}}{the proportion of samples retained for evaluation in `luz}

\item{\code{dropout}}{the dropout rate for \verb{monte carlo dropout}, i.e. the proportion of neurons dropped in each layer (must be between 0.1 and 0.5)}

\item{\code{batch_size}}{the mini-batch size}

\item{\code{epochs}}{the number of epochs}

\item{\code{early_stopping}}{logical, whether to use early stopping or not}

\item{\code{verbose}}{logical, whether to print messages and progress bars}

\item{\code{patience}}{the patience (number of iterations) before early stopping}

\item{\code{optimizer}}{a "torch_optimizer_generator", the optimizer to use in \code{luz} (default=optim_adam)}

\item{\code{learning_rate}}{the learning rate}

\item{\code{l2_weight_decay}}{the L2 weigth decay value for L2 regularization}

\item{\code{variance_clamping}}{clamp all elements in the network weigths in the range \link{ min, max }. See \code{torch_clamp()}.}

\item{\code{loss}}{a custom loss function passed to the ``monte carlo dropout` method (default=nn_mse_loss())}

\item{\code{tol}}{the tolerance rate in \code{abc} for the \code{tabnet-abc} method (\code{tolerance}). The required proportion of points accepted nearest the target values.}

\item{\code{abc_method}}{a character string indicating the type of ABC algorithm to be applied. Possible values are "rejection", "loclinear", "neuralnet" and "ridge".}

\item{\code{num_posterior_samples}}{the number of posterior samples to predict with the \verb{concrete dropout} and \verb{monte carlo dropout} methods}

\item{\code{prior_length_scale}}{the prior length scale hyperparameter for the \verb{concrete dropout} method}

\item{\code{epsilon_adversarial}}{the multiplying factor for perturbed inputs in adversarial training in the \verb{deep ensemble} method (EXPERIMENTAL). The perturbation is a random noise in the range \link{-beta, beta}, with beta = epsilon_adversarial * variance. Set NULL to disable adversarial training (default). Otherwise 0.01 is a good value to start with.}

\item{\code{ncores}}{integer, the number of cores in parallel operations}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-fit"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-fit}{}}}
\subsection{Method \code{fit()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$fit()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-predict"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-predict}{}}}
\subsection{Method \code{predict()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$predict(data = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{data}}{a new set of data to predict

Prepare the torch dataloader from sumstat/theta (input/target)

Build and return a dataloader object

Estimate a calibrated credible interval with Conformal Prediction

Returns a tidy tibble with predictions and credible intervals

Print a summary of the \code{abcnn} object

Plot the training curves (training/validation)

Plot predicted values and their credible intervals}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-dataloader"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-dataloader}{}}}
\subsection{Method \code{dataloader()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$dataloader()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-conformal_prediction"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-conformal_prediction}{}}}
\subsection{Method \code{conformal_prediction()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$conformal_prediction()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-predictions"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-predictions}{}}}
\subsection{Method \code{predictions()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$predictions()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-summary"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-summary}{}}}
\subsection{Method \code{summary()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$summary()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-plot_training"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-plot_training}{}}}
\subsection{Method \code{plot_training()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$plot_training(discard_first = FALSE)}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-plot_prediction"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-plot_prediction}{}}}
\subsection{Method \code{plot_prediction()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$plot_prediction(uncertainty_type = "conformal", plot_type = "line")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{uncertainty_type}}{The type of uncertainty to plot, whether \code{conformal} credible intervals (default),
the \code{uncertainty} estimated (square root of the variance) or the \verb{posterior quantile}, that are credible intervals
computed on the distribution of posteriors.}

\item{\code{plot_type}}{The type of plot, whether a \code{line} or \code{errorbar} around points

Plot the distributions of estimates and predictions}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-plot_posterior"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-plot_posterior}{}}}
\subsection{Method \code{plot_posterior()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$plot_posterior(sample = 1, prior = TRUE, uncertainty_type = "conformal")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{prior}}{logical, whether to plot the prior underneath the posterior and prediction}

\item{\code{uncertainty_type}}{The type of uncertainty to plot, whether \code{conformal} credible intervals (default),
the \code{uncertainty} estimated (square root of the variance) or the \verb{posterior quantile}, that are credible intervals
computed on the distribution of posteriors.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-abcnn-clone"></a>}}
\if{latex}{\out{\hypertarget{method-abcnn-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{abcnn$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
