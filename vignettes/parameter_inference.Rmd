---
title: "model_inference"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{model_inference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# library(abcneuralnet)
library(ggplot2)
library(torch)
library(keras3)
```


## Principles of Bayesian Neural Network inference


### Toy dataset - a simple linear regression problem


```{r, echo = T}
n_train = 2000 # Number of data points
n_obs = 1000 # Validation size

gen_data_1d = function(n) {
  sigma = 1
  X = matrix(rnorm(n))
  w = 2
  b = 8
  Y = matrix(X %*% w + b + sigma * rnorm(n))
  list(X, Y)
}

c(X, Y) %<-% gen_data_1d(n_train + n_obs)

c(X_train, Y_train) %<-% list(X[1:n_train], Y[1:n_train])
c(X_obs, Y_obs) %<-% list(X[(n_train + 1):(n_train + n_obs)], 
                          Y[(n_train + 1):(n_train + n_obs)])


# Predict Y when X is observed
theta = data.frame(y1 = Y_train)
sumstats = data.frame(x1 = X_train)
observed = data.frame(X1 = X_obs)
```

### Fit a model with `abcnn`

We use a Concrete Dropout regression model described in [ref]. See https://blogs.rstudio.com/ai/posts/2018-11-12-uncertainty_estimates_dropout/ for more details.


```{r, echo = T}
# Init an abcnn object with inputs and targets
abc = abcnn$new(theta,
            sumstats,
            observed,
            method = 'concrete dropout',
            num_hidden_layers = 3,
            num_hidden_dim = 256,
            epochs = 30,
            l2_weight_decay = 1e-5)


# abc$summary()

# Plot simulation distribution and observed data points
# abc$plot_lda()
```

The `abcnn` object is a `R6Class` object.
Modern implementation, OOP


For example, the fitted model can be accessed with `abc$model`.


The model is trained by calling `abc$fit()`.


```{r, echo = T}
# Use the fit() method to train the neural network
abc$fit()
```

Don't worry, sometimes the loss can be negative. It is normal and does not impact the training procedure.

Check model fit.

```{r, echo = T}
# Check the fit of the model
as.numeric(unlist(abc$fitted$records$metrics$train))
as.numeric(unlist(abc$fitted$records$metrics$valid))
abc$eval_metrics

# The dropout rate
abc$dropout_rates # Not yet implemented
```



Plot the training curve. The black horizontal line is the loss computed on the test dataset.


```{r, echo = T}
train_metric = as.numeric(unlist(abc$fitted$records$metrics$train))
valid_metric = as.numeric(unlist(abc$fitted$records$metrics$valid))
eval = abc$eval_metrics$value


train_eval = data.frame(Epoch = rep(1:length(train_metric), 2),
                        Metric = c(train_metric, valid_metric),
                        Mode = c(rep("train", length(train_metric)), rep("validation", length(valid_metric))))

ggplot(train_eval, aes(x = Epoch, y = Metric, color = Mode, fill = Mode)) +
  geom_point() +
  geom_line() +
  xlab("Epoch") + ylab("Loss") +
  geom_hline(yintercept = eval) +
  theme_bw()
```



```{r, echo = T}
abc$plot_training()
```





Save the `abccn` object with the fitted model.


```{r, echo = T}
save_abcnn(abc, prefix = "../inst/extdata/abc_concrete")

abc = load_abcnn(prefix = "../inst/extdata/abc_concrete")
```





## Parameter inference from observed summary statistics


After the model is trained, parameter estimates can be predicted with `abc$predict()`.



```{r, echo = T}
abc$predict()
```


Model predictions.

```{r, echo = T}
# Return a tidy data frame
head(abc$predictions())
```



Predictions for the test dataset. Here I can plot the true Y value since we used a simulated observed dataset.


```{r, echo = T}
df_predicted = abc$predictions()

df_predicted$ci_overall_upper = df_predicted$Predictive_mean + df_predicted$Overall_uncertainty
df_predicted$ci_overall_lower = df_predicted$Predictive_mean - df_predicted$Overall_uncertainty

df_predicted$ci_e_upper = df_predicted$Predictive_mean + df_predicted$Epistemic_uncertainty
df_predicted$ci_e_lower = df_predicted$Predictive_mean - df_predicted$Epistemic_uncertainty

df_predicted$ci_conformal_upper = df_predicted$Predictive_mean + df_predicted$Overall_conformal_credible_interval
df_predicted$ci_conformal_lower = df_predicted$Predictive_mean - df_predicted$Overall_conformal_credible_interval

df_predicted$ci_conformal_e_upper = df_predicted$Predictive_mean + df_predicted$Epistemic_conformal_credible_interval
df_predicted$ci_conformal_e_lower = df_predicted$Predictive_mean - df_predicted$Epistemic_conformal_credible_interval

df_predicted$x = X_obs
df_predicted$y_true = Y_obs

df_training = data.frame(x = X_train,
                         y = Y_train)

ggplot(data = df_training, aes(x = x, y = y)) +
  geom_point(color = "blue", alpha = 0.3) +
  # geom_point(data = df_predicted, aes(x = x, y = y_true), color = "green", alpha = 0.3) +
  geom_line(data = df_predicted, aes(x = x, y = Predictive_mean), color = "Red") +
  geom_point(data = df_predicted, aes(x = x, y = Predictive_mean), color = "Red") +
  facet_wrap(~ Parameter, scales = "free") +
  geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_conformal_e_upper, ymax = ci_conformal_e_lower), alpha = 0.4, fill = "purple") +
  geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_conformal_upper, ymax = ci_conformal_lower), alpha = 0.3, fill = "green") +
  geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_overall_lower, ymax = ci_overall_upper), alpha = 0.3, fill = "red") +
  geom_ribbon(data = df_predicted, aes(x = x, y = Predictive_mean, ymin = ci_e_lower, ymax = ci_e_upper), alpha = 0.3, fill = "red") +
  theme_bw()
```


Alternatively you can use the `plot_predicted()` method.


```{r, echo = T}
# predicted (+ C.I.) ~ observed
abc$plot_predicted(paired = TRUE, type = "uncertainty")

abc$plot_predicted(paired = TRUE, type = "conformal")

abc$plot_predicted(paired = TRUE, type = "posterior")
```



To focus on a single prediction, you can look at the distribution of approximate posterior estimates with predictive mean and credible intervals. Compared with prior distribution.


```{r, echo = T}
# Dim 1 is number of MC samples (predictions)
# Dim 2 is number of observations
# Dim 3 is parameters (mu + sigma)
i = 500 # The sample to plot
posteriors = abc$posterior_samples[,i,]
# output_names = unlist(lapply(c("mu", "sigma"), function(x) paste(colnames(theta), x, sep = "_")))
posteriors = as.data.frame(posteriors)
posteriors = posteriors[,1:abc$output_dim]
posteriors = as.data.frame(posteriors)
colnames(posteriors) = colnames(abc$theta)
posteriors$mc_sample = as.character(c(1:nrow(posteriors)))

tidy_df = posteriors %>% tidyr::gather(param, prediction, any_of(colnames(abc$theta)))
# tidy_df



tidy_predictions = abc$predictions()
tidy_predictions = tidy_predictions[tidy_predictions$Sample == i,]

tidy_predictions$ci_upper = tidy_predictions$Predictive_mean + tidy_predictions$Overall_uncertainty
tidy_predictions$ci_lower = tidy_predictions$Predictive_mean - tidy_predictions$Overall_uncertainty

tidy_predictions$ci_e_upper = tidy_predictions$Predictive_mean + tidy_predictions$Epistemic_uncertainty
tidy_predictions$ci_e_lower = tidy_predictions$Predictive_mean - tidy_predictions$Epistemic_uncertainty

tidy_predictions$ci_conformal_e_upper = tidy_predictions$Predictive_mean + tidy_predictions$Epistemic_conformal_credible_interval
tidy_predictions$ci_conformal_e_lower = tidy_predictions$Predictive_mean - tidy_predictions$Epistemic_conformal_credible_interval

tidy_predictions$ci_conformal_upper = tidy_predictions$Predictive_mean + tidy_predictions$Overall_conformal_credible_interval
tidy_predictions$ci_conformal_lower = tidy_predictions$Predictive_mean - tidy_predictions$Overall_conformal_credible_interval

# abc$theta
tidy_priors = data.frame(param = rep(colnames(abc$theta), each = nrow(abc$theta)),
                         prior = as.numeric(unlist(abc$theta)))
# tidy_priors

pal = RColorBrewer::brewer.pal(8, "Dark2")

cols = c("Epistemic"=pal[1],"Overall"=pal[2], "Epistemic conformal"=pal[3], "Overall conformal"=pal[4])

ggplot() +
  geom_histogram(data = tidy_priors, aes(x = prior), color = "darkgrey", fill = "grey", alpha = 0.1) +
  geom_histogram(data = tidy_df, aes(x = prediction)) +
  geom_vline(data = tidy_predictions, aes(xintercept = Predictive_mean, colour = "Epistemic")) +
  geom_rect(data = tidy_predictions, aes(xmin = ci_e_lower, xmax = ci_e_upper, ymin = -Inf, ymax = Inf, colour = "Epistemic", fill = "Epistemic"), alpha = 0.1) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_e_lower, colour = "Epistemic")) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_e_upper, colour = "Epistemic")) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_lower, colour = "Overall")) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_upper, colour = "Overall")) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_conformal_lower, colour = "Overall conformal")) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_conformal_upper, colour = "Overall conformal")) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_conformal_e_lower, colour = "Epistemic conformal")) +
  geom_vline(data = tidy_predictions, aes(xintercept = ci_conformal_e_upper, colour = "Epistemic conformal")) +
  geom_rect(data = tidy_predictions, aes(xmin = ci_lower, xmax = ci_upper, ymin = -Inf, ymax = Inf, colour = "Overall", fill = "Overall"), alpha = 0.1) +
  geom_rect(data = tidy_predictions, aes(xmin = ci_conformal_lower, xmax = ci_conformal_upper, ymin = -Inf, ymax = Inf, colour = "Overall conformal", fill = "Overall conformal"), alpha = 0.1) +
  geom_rect(data = tidy_predictions, aes(xmin = ci_conformal_e_lower, xmax = ci_conformal_e_upper, ymin = -Inf, ymax = Inf, colour = "Epistemic conformal", fill = "Epistemic conformal"), alpha = 0.1) +
  facet_wrap(~ Parameter, scales = "free") +
  scale_colour_manual(name = "Uncertainty", values = cols) +
  scale_fill_manual(name = "Uncertainty", values = cols) +
  xlab("Value") + ylab("Count") +
  theme_bw() +
  theme(legend.position = "right")
```


Try to compare a prediction within the training data with a prediction out of prior distribution. See the effect of random noise or out of distribution on uncertainty.


```{r, echo = T}
# Print a sample with -5 < x1 < -4 (within the distribution with a low noise)
# which(abc$observed$X1 < 2 & abc$observed > -2)
abc$plot_posterior(sample = 501, prior = TRUE) +
  geom_vline(xintercept = Y_obs[501], color = "red", size = 1.5)
Y_obs[501]
abc$predictive_mean$y1[501]

# Print a sample out of distribution
# which(abc$observed$X1 > 7)
abc$plot_posterior(sample = 735, prior = TRUE) +
  geom_vline(xintercept = Y_obs[735], color = "red", size = 1.5)
Y_obs[735]
abc$predictive_mean$y1[735]
```













## Deep Ensemble prediction


### A more complex relationship (sinus and cosinus functions)

Two parameters to estimate.

Simulate two different functions (sin and cos), with missing data (gaps) and different amounts of random noise in the two parts of the dataset.

```{r, echo = T}
# Parameters of simulated input x
data_range = 7
data_step = 0.001

# Boundaries of the gap in the data range
bound1 = -2
bound2 = 2

# Random noise applied on y
data_sigma1a = 0.1
data_sigma2a = 0.5

data_sigma1b = 0.2
data_sigma2b = 0.1

# Number of simulated data points
# num_data = 10000

# Simulate x1
data_x1a = seq(-data_range, bound1 + data_step, by = data_step)
data_x1b = seq(bound2, data_range + data_step, by = data_step)
# Simulate targets y
data_y1a = sin(data_x1a) + rnorm(length(data_x1a), 0, data_sigma1a)
data_y1b = sin(data_x1b) + rnorm(length(data_x1b), 0, data_sigma2a)

# Shift X1 to get X2
data_x2a = data_x1a + 7
data_x2b = data_x1b + 7
# Simulate targets y
data_y2a = cos(data_x2a) + rnorm(length(data_x2a), 0, data_sigma1b)
data_y2b = cos(data_x2b) + rnorm(length(data_x2b), 0, data_sigma2b)

df = data.frame(x1 = c(data_x1a, data_x1b),
                    x2 = c(data_x2a, data_x2b),
                    y1 = c(data_y1a, data_y1b),
                    y2 = c(data_y2a, data_y2b))

# Shuffle data
shuffle_idx = sample(1:(nrow(df)), nrow(df), replace = FALSE)
df_train = df[shuffle_idx,]

# Train/Test datasets
# test_ratio = 0.1
# num_train_data = round(nrow(df) * (1 - test_ratio), digits = 0)
# num_test_data  = nrow(df) - num_train_data
# 
# train_x = df[1:num_train_data, c("x1", "x2")]
# train_y = df[1:num_train_data, c("y1", "y2")]
# test_x = df[num_train_data:nrow(df_train), c("x1", "x2")]
# test_y = df[num_train_data:nrow(df_train), c("y1", "y2")]
train_x = df_train[, c("x1", "x2")]
train_y = df_train[, c("y1", "y2")]


# Make a pseudo-obseerved dataset with out of distribution data points
# Simulate x1
data_x1 = seq(-data_range, data_range, length.out = 1000)
# Simulate true targets y
data_y1 = sin(data_x1)

# Shift X1 to get X2
data_x2 = data_x1 + 7
# Simulate targets y
data_y2 = cos(data_x2)

df_observed = data.frame(x1 = data_x1,
                x2 = data_x2,
                y1 = data_y1,
                y2 = data_y2)


observed_x  = df_observed[, c("x1", "x2")]
observed_y  = df_observed[, c("y1", "y2")]
```


```{r, echo = F}
# Plot the simulated data
p1 = ggplot(data = df_train, aes(x = x1, y = y1)) +
  geom_point(color = "Blue", alpha = 0.2) +
  geom_line(data = df_observed, aes(x = x1, y = y1), color = "red") +
  theme_bw()

p2 = ggplot(data = df_train, aes(x = x2, y = y2)) +
  geom_point(color = "Green", alpha = 0.2) +
  geom_line(data = df_observed, aes(x = x2, y = y2), color = "red") +
  theme_bw()

ggpubr::ggarrange(p1, p2, ncol = 2)
```




### Fit a Deep Ensemble network

Train five networks with the Deep Ensemble algorithm and adversarial training.

Try with a single parameter to estimate first.


```{r, echo = T}
# Predict Y when X is observed
theta = data.frame(y = train_y$y1)
sumstats = data.frame(x = train_x$x1)
observed = data.frame(x = observed_x$x1)

abc_ensemble = abcnn$new(theta,
            sumstats,
            observed,
            method = 'deep ensemble',
            num_networks = 5,
            epochs = 30,
            num_hidden_layers = 3,
            num_hidden_dim = 500,
            batch_size = 256,
            epsilon_adversarial = 0.01)

# The perturbation on input
abc_ensemble$epsilon_adversarial * 2 * abc_ensemble$sumstat_sd

abc_ensemble$fit()
```



```{r, echo = T}
save_abcnn(abc_ensemble, prefix = "../inst/extdata/abc_ensemble")

abc_ensemble = load_abcnn(prefix = "../inst/extdata/abc_ensemble")
```




```{r, echo = T}
abc_ensemble$plot_training()
```




````{r, echo = T}
abc_ensemble$predict()

abc_ensemble$plot_predicted(paired = TRUE) +
  geom_point(data = df_train, aes(x = x1, y = y1), color = "blue", alpha = 0.01)
```



For Deep Ensemble, there are no posterior samples to plot with the `plot_posterior` method as in Monte Carlo dropout or Concrete Dropout. But it is still interesting to check a single prediction with its uncertainty within or without training distribution.

```{r, echo = T}
# Print a sample with -5 < x1 < -4 (within the distribution with a low noise)
# which(abc$observed < -4 & abc$observed > -5)
abc_ensemble$plot_posterior(sample = 155, prior = TRUE)

# Print a sample with 4 < x1 < 5 (within the distribution with a high noise)
# which(abc$observed < 5 & abc$observed > 4)
abc_ensemble$plot_posterior(sample = 800, prior = TRUE)

# Print a sample with -1 < x1 < 1 (out of training distribution)
# which(abc$observed < 1 & abc$observed > -1)
abc_ensemble$plot_posterior(sample = 520, prior = TRUE)
```



### Multi parameter inference

And for two parameters jointly estimated?



```{r, echo = T}
# Predict Y when X is observed
theta = train_y
sumstats = train_x
observed = observed_x

abc_ensemble = abcnn$new(theta,
            sumstats,
            observed,
            method = 'deep ensemble',
            num_networks = 5,
            epochs = 30,
            num_hidden_layers = 3,
            num_hidden_dim = 500,
            batch_size = 256,
            epsilon_adversarial = 0.01)

# The perturbation on input
abc_ensemble$epsilon_adversarial * 2 * abc_ensemble$sumstat_sd

abc_ensemble$fit()
```



```{r, echo = T}
save_abcnn(abc_ensemble, prefix = "../inst/extdata/abc_ensemble_2param")

abc_ensemble = load_abcnn(prefix = "../inst/extdata/abc_ensemble_2param")
```




```{r, echo = T}
abc_ensemble$plot_training()
```




````{r, echo = T}
abc_ensemble$predict()

abc_ensemble$plot_predicted(paired = TRUE)
```




```{r, echo = T}
# Print a sample with -5 < x1 < -4 (within the distribution with a low noise)
# which(abc$observed < -4 & abc$observed > -5)
abc_ensemble$plot_posterior(sample = 155, prior = TRUE)

# Print a sample with 4 < x1 < 5 (within the distribution with a high noise)
# which(abc$observed < 5 & abc$observed > 4)
abc_ensemble$plot_posterior(sample = 800, prior = TRUE)

# Print a sample with -1 < x1 < 1 (out of training distribution)
# which(abc$observed < 1 & abc$observed > -1)
abc_ensemble$plot_posterior(sample = 520, prior = TRUE)
```




## Combine ABC and Bayesian Neural Networks to estimate population genetics parameters

Simulations with `msprime`.

Reference table (simulations summary stats) and priors.


```{r, echo = T}
# Import the reference table and other data
sumstats = read.table("../inst/extdata/reference_table_full.csv",
                             sep = ",", header = T, row.names = NULL)

ncol(sumstats)
colnames(sumstats)

row.names(sumstats) = sumstats$sample
sumstats = sumstats[,-1]
colnames(sumstats)[1]

theta_sim = read.table("../inst/extdata/theta_simulations_full.csv",
                    sep = "\t", header = T, fill = TRUE)
head(theta_sim)
row.names(theta_sim) = theta_sim$sample
theta_sim = theta_sim[,-1]
colnames(theta_sim)[1]

# Check consistency between priors and reference table
nrow(sumstats) == nrow(theta_sim)
sum(row.names(sumstats) == row.names(theta_sim)) == nrow(theta_sim)


# Split datasets
# Two parameters, t1 and t2
idx_training = c(1:(nrow(sumstats) - 1000))
idx_observed = c((nrow(sumstats) - 999):nrow(sumstats))

theta_training = theta_sim[idx_training,c("t1", "t2")]
theta_observed = theta_sim[idx_observed,c("t1", "t2")]


sumstats_training = sumstats[idx_training,]
sumstats_observed = sumstats[idx_observed,]

# colnames(theta_training)
```



Train with full data.


```{r, echo = T}
abc_popgen = abcnn$new(theta_training,
            sumstats_training,
            sumstats_observed,
            method = 'concrete dropout',
            num_hidden_layers = 3,
            num_hidden_dim = 512,
            epochs = 30,
            early_stopping = TRUE,
            patience = 2,
            scale = TRUE)

abc_popgen$fit()

abc_popgen$plot_training()
# With Concrete dropout, the first training loss can be huge compared to subsequent steps
# Remove it from the plot to better see training curves
abc_popgen$plot_training(discard_first = TRUE)
```


```{r, echo = T}
abc_popgen$predict()

abc_popgen$plot_predicted()
```


```{r, echo = T}
save_abcnn(abc_popgen, prefix = "../inst/extdata/abc_popgen")

abc_popgen = load_abcnn(prefix = "../inst/extdata/abc_popgen")
```



```{r, echo = T}
abc_popgen$plot_posterior(100)

abc_popgen$predictive_mean[100,]
theta_observed[100,]

# Plot the best estimate (lower C.I)
idx = which.min(abc_popgen$epistemic_uncertainty[,2])
idx
true_theta = data.frame(Parameter = colnames(theta_observed),
                        Value = unlist(c(theta_observed[idx,])))
true_theta

abc_popgen$plot_posterior(idx) +
  geom_vline(data = true_theta, aes(xintercept = Value), color = "red")


# Plot the worst estimate (larger C.I.)
idx = which.max(abc_popgen$epistemic_uncertainty[,2])
idx
true_theta = data.frame(Parameter = colnames(theta_observed),
                        Value = unlist(c(theta_observed[idx,])))
true_theta

abc_popgen$plot_posterior(idx) +
  geom_vline(data = true_theta, aes(xintercept = Value), color = "red")
```




Train with a Deep Ensemble.



```{r, echo = T}
abc_popgen = abcnn$new(theta_training,
            sumstats_training,
            sumstats_observed,
            method = 'deep ensemble',
            scale = TRUE,
            learning_rate = 0.0001,
            num_hidden_layers = 3,
            num_hidden_dim = 512,
            batch_size = 390,
            epochs = 30,
            epsilon_adversarial = 0.01)


abc_popgen$fit()

abc_popgen$plot_training()
```


```{r, echo = T}
abc_popgen$predict()

abc_popgen$plot_predicted()
```


```{r, echo = T}
# save_abcnn(abc_popgen, prefix = "../inst/extdata/abc_popgen_ensemble")
# TODO Error external pointer not found
# abc_popgen = load_abcnn(prefix = "../inst/extdata/abc_popgen_ensemble")
```



```{r, echo = T}
abc_popgen$plot_posterior(100)

abc_popgen$predictive_mean[1:10,]
theta_observed[1:10,]

idx = 100
true_theta = data.frame(param = colnames(theta_observed),
                        value = unlist(c(theta_observed[idx,])))

abc_popgen$plot_posterior(idx) +
  geom_vline(data = true_theta, aes(xintercept = value))


# Plot the best estimate (lower C.I)
idx = which.min(as.numeric(abc_popgen$epistemic_uncertainty[,1]))
true_theta = data.frame(param = colnames(theta_observed),
                        value = unlist(c(theta_observed[idx,])))

abc_popgen$plot_posterior(idx) +
  geom_vline(data = true_theta, aes(xintercept = value))

# Plot the worst estimate (larger C.I.)
idx = which.max(as.numeric(abc_popgen$epistemic_uncertainty[,1]))
true_theta = data.frame(param = colnames(theta_observed),
                        value = unlist(c(theta_observed[idx,])))

abc_popgen$plot_posterior(idx) +
  geom_vline(data = true_theta, aes(xintercept = value))
```


Train with ABC Importance sampling.





